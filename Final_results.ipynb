{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ac42b5",
   "metadata": {},
   "source": [
    "Here we are fectching API key from environmental variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "api_key_=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259ee49",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2bfb8",
   "metadata": {},
   "source": [
    "In the below cell we are using upload button for the resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a54a87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85de78461f034a32b297840833206263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), description='Upload')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import FileUpload\n",
    "\n",
    "upload_button = FileUpload()\n",
    "upload_button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcfde22",
   "metadata": {},
   "source": [
    "In the below cells, we are extracting all the content from the uploaded resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a16b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_files = upload_button.value\n",
    "\n",
    "if uploaded_files:\n",
    "    file_name = next(iter(uploaded_files))\n",
    "    file_content = uploaded_files[0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92f67c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Content from file upload:\n",
      " Tarun Sai Pamulapati\n",
      "     Fort Collins, CO|(970) 889 5731| pamulapatitarunsai@gmail.com| LinkedIn| GitHub| LeetCode\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "Masterâ€™s in Computer Science, Colorado State University\t                     Fort Collins, CO  \n",
      "Coursework: Big Data Analytics, Data Mining, Machine Learning, Statistical Analysis, Predictive Modeling\t \t                                    GPA: 4.0/4.0\n",
      "Bachelorâ€™s in Information Technology, Sagi Rama Krishnam Raju Engineering College\t               Bhimavaram, India\n",
      "Coursework: Data Warehousing & Business Intelligence, DBMS, Data Structures & Algorithms\t \t                                                      GPA: 3.6/4.0\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Research Assistant| Colorado State University| Fort Collins, CO                                                                                                    Feb 2023 â€“ May 2024\n",
      "Analyzed Urban Heat Island (UHI) index from Landsat 8 & 9 with OpenStreetMap data to study urban heat patterns in Fort Collins, CO (2021â€“2022).\n",
      "Processed 10 years of hyperspectral satellite data across California using ArcGIS to predict soil texture using ML models.\n",
      "Used autoencoders to reduce 200+ spectral bands from hyperspectral imagery while preserving key features.\n",
      "Combined encoded data with POLARIS soil maps to train regression models on sand, silt, and clay properties.\n",
      "Demonstrated scalable, remote sensing-based approach for environmental analysis beyond conventional methods.\n",
      "Data Analyst| TEKsystems Global Services| India\t         Aug 2021 â€“ Dec 2022\n",
      "Designed and optimized 10+ Power BI dashboards for financial analytics, reducing manual report generation time by 40% and enabling real-time KPI tracking.\n",
      "Automated ETL workflows using Azure Data Factory, reducing manual data handling by 80%, and improving data pipeline efficiency by 45%.\n",
      "Optimized SQL queries (CTEs, window functions, indexing, partitioning) for financial databases, reducing query run times by 50%.\n",
      "Transformed unstructured legacy system data into a star-schema warehouse, improving reporting efficiency by 60%.\n",
      "Collaborated with business stakeholders and finance teams to develop analytical reports that improved budget allocation decisions by 20%.\n",
      "Data Analyst Intern| Sri Maharshi Consultancy| India\t        Aug 2019 â€“ Feb 2020\n",
      "Designed a banking queue management system, reducing customer wait times by 40% through real-time data analytics.\n",
      "Built interactive Power BI dashboards to track service duration metrics, leading to a 30% reduction in customer drop-off rates.\n",
      "Improved customer experience by conducting sentiment analysis on waiting times, driving a 25% increase in customer satisfaction scores.\n",
      "Developed and automated 5+ revenue reports to analyze customer visit frequency and service preferences, enabling personalized banking recommendations that boosted cross-selling conversion rates by 20%.\n",
      "Web Development Intern| MCR Web Solutions| India\t      Jun 2018 â€“ May 2019\n",
      "Led the development of product pages for www.bhimavaram.online, an e-commerce platform built using OpenCart and Bootstrap.\n",
      "Increased user engagement by 25% by redesigning the e-commerce website's UX/UI and improving navigation flow.\n",
      "Optimized front-end components and improved mobile responsiveness reducing bounce rates by 20%.\n",
      "\n",
      "TECHNICAL SKILLS\n",
      "\n",
      "Data Analysis & Visualization\t: Power BI, Tableau, Microsoft Excel, matplotlib, Seaborn\n",
      "SQL & Database\t: MySQL, Oracle, MongoDB, PostgreSQL, Cassandra, SQL Server\n",
      "Big Data & Cloud Technologies\t: Apache Spark, Hadoop HDFS, AWS (Glue, Redshift, Athena, S3), Azure Synapse, Event Hubs,  Apache Airflow\n",
      "Programming & Statistical Modeling\t: C, Java, Python, R, SQL, C++, JavaScript, HTML, CSS, PHP, Bootstrap, Machine Learning  \n",
      "ETL & Data Engineering\t: Data Warehousing, Feature Engineering, Stream Processing, Event Hubs\n",
      "\n",
      "CERTIFICATIONS\n",
      "\n",
      "Microsoft Certified: Azure Data Engineer Associate - Microsoft\n",
      "Data Engineering with AWS Nanodegree - Udacity\n",
      "Big Data Specialization (UC San Diego) - Coursera                                                                                                       \n",
      "Microsoft Azure Data Fundamentals - Microsoft                                                                                                     \n",
      "Full Stack Developer - Simplilearn                                                                                                                               \n",
      "Power BI A-Z: Hands-On Power BI offered by Udemy          \n",
      "\n",
      "PROJECTS\n",
      "\n",
      "Predictive Analytics for SEC Insider Trading [Spark | Storm | Java | HDFS | PyTorch | Python]                                              \n",
      "Developed a reinforcement learning model to analyze SEC Form 4 insider trading data for predicting short-term trading opportunities.\n",
      "Implemented feature engineering techniques, achieving a 10% increase in risk-adjusted returns through back-testing.\n",
      "Optimized Music Recommendation System [Java | Spark | HDFS | Python]           \n",
      "Developed a bias-reducing music recommendation system using ALS collaborative filtering, improving discovery of emerging artists by 18%.\n",
      "Performed A/B testing to evaluate performance, leading to a more balanced recommendation strategy.\n",
      "Wikipedia Page Rank Analysis [Java | HDFS | MapReduce]                                                                       \n",
      "Designed and implemented a scalable PageRank algorithm for ranking Wikipedia pages, reducing computation time by 35%.\n",
      "Utilized big data processing techniques to refine ranking accuracy by 25%.\n",
      "Sales Forecasting & Revenue Optimization [Python | SQL | Tableau]                                                                         \n",
      "Built a time series forecasting model (ARIMA, LSTM) to predict monthly sales trends, improving revenue estimation by 30%.\n",
      "Developed a Power BI dashboard to visualize real-time sales insights and inventory trends, reducing overstock by 15%.\n",
      "Plant Disease Detection using CNN [HTML | CSS | Flask | Python | CNN]                                                                          \n",
      "Developed a convolutional neural network (CNN) for classifying plant diseases with 95% accuracy.\n",
      "Improved prediction reliability by optimizing hyperparameters and reducing false positives by 30%.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "import io\n",
    "import fitz \n",
    "\n",
    "def read_docx_from_memory(file_data):\n",
    "    doc = Document(io.BytesIO(file_data))\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "def read_pdf_from_memory(file_data):\n",
    "    file_bytes = bytes(file_data)\n",
    "    doc = fitz.open(stream=file_bytes, filetype=\"pdf\")\n",
    "    text = \"\\n\".join([page.get_text() for page in doc])\n",
    "    return text\n",
    "\n",
    "def read_txt_from_memory(file_data):\n",
    "    return bytes(file_data).decode(\"utf-8\")\n",
    "\n",
    "if file_name['name'].endswith(\".docx\"):\n",
    "    file_content_text = read_docx_from_memory(file_content)\n",
    "elif file_name['name'].endswith(\".pdf\"):\n",
    "    file_content_text = read_pdf_from_memory(file_content)\n",
    "elif file_name['name'].endswith(\".txt\"):\n",
    "    file_content_text = read_txt_from_memory(file_content)\n",
    "else:\n",
    "    file_content_text = \"Unsupported file format.\"\n",
    "\n",
    "print(\"Extracted Content from file upload:\\n\", file_content_text[:20000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092bb7b6",
   "metadata": {},
   "source": [
    "In the below cell, we are extracting only names of the skills that are present in each of the uploaded resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "faf90620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the extracted skills from the resume:\n",
      "\n",
      "1. Power BI\n",
      "2. Tableau\n",
      "3. Microsoft Excel\n",
      "4. matplotlib\n",
      "5. Seaborn\n",
      "6. MySQL\n",
      "7. Oracle\n",
      "8. MongoDB\n",
      "9. PostgreSQL\n",
      "10. Cassandra\n",
      "11. SQL Server\n",
      "12. Apache Spark\n",
      "13. Hadoop HDFS\n",
      "14. AWS (Glue, Redshift, Athena, S3)\n",
      "15. Azure Synapse\n",
      "16. Event Hubs\n",
      "17. Apache Airflow\n",
      "18. C\n",
      "19. Java\n",
      "20. Python\n",
      "21. R\n",
      "22. SQL\n",
      "23. C++\n",
      "24. JavaScript\n",
      "25. HTML\n",
      "26. CSS\n",
      "27. PHP\n",
      "28. Bootstrap\n",
      "29. Machine Learning\n",
      "30. Data Warehousing\n",
      "31. Feature Engineering\n",
      "32. Stream Processing\n",
      "33. Event Hubs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=api_key_)\n",
    "question= f\"Extract only name of the skills from the uploaded resume: {file_content_text[:20000]}\"\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ee7a6",
   "metadata": {},
   "source": [
    "We are storing all of those skills in the temporary_skill_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0685187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Skills:\n",
      " ['Python', 'JavaScript', 'Tableau', 'SQL', 'Event Hubs', 'Oracle', 'SQL Server', 'Java', 'Cassandra', 'C++', 'CSS', 'Apache Spark', 'HTML', 'AWS (Glue', 'MongoDB', 'Feature Engineering', 'Power BI', 'Azure Synapse', 'Stream Processing', 'Machine Learning', 'matplotlib', 'PostgreSQL', 'S3)', 'Seaborn', 'Bootstrap', 'Hadoop HDFS', 'Data Warehousing', 'Microsoft Excel', 'Here are the extracted skills from the resume:', 'Redshift', 'R', 'Athena', 'MySQL', 'C', 'Apache Airflow', 'PHP']\n"
     ]
    }
   ],
   "source": [
    "def extract_skills_to_list(response):\n",
    "    skills = []\n",
    "    lines = response.splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        cleaned = line.strip(\"-â€¢*0123456789. \\t\")\n",
    "        if \",\" in cleaned:\n",
    "            parts = [skill.strip() for skill in cleaned.split(\",\")]\n",
    "            skills.extend(parts)\n",
    "        elif cleaned:\n",
    "            skills.append(cleaned)\n",
    "    return list(set(skill for skill in skills if skill))\n",
    "temporary_skill_list = extract_skills_to_list(completion.choices[0].message.content)\n",
    "\n",
    "print(\"\\nExtracted Skills:\\n\", temporary_skill_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e19e0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"person3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016b3be",
   "metadata": {},
   "source": [
    "For each person, we are browsing their GitHub account in order to fetch skills of that person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa339ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'login': 'tarunsaipamulapati', 'id': 132961621, 'node_id': 'U_kgDOB-zVVQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/132961621?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/tarunsaipamulapati', 'html_url': 'https://github.com/tarunsaipamulapati', 'followers_url': 'https://api.github.com/users/tarunsaipamulapati/followers', 'following_url': 'https://api.github.com/users/tarunsaipamulapati/following{/other_user}', 'gists_url': 'https://api.github.com/users/tarunsaipamulapati/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/tarunsaipamulapati/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/tarunsaipamulapati/subscriptions', 'organizations_url': 'https://api.github.com/users/tarunsaipamulapati/orgs', 'repos_url': 'https://api.github.com/users/tarunsaipamulapati/repos', 'events_url': 'https://api.github.com/users/tarunsaipamulapati/events{/privacy}', 'received_events_url': 'https://api.github.com/users/tarunsaipamulapati/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False, 'name': None, 'company': None, 'blog': '', 'location': None, 'email': None, 'hireable': None, 'bio': None, 'twitter_username': None, 'public_repos': 67, 'public_gists': 0, 'followers': 1, 'following': 0, 'created_at': '2023-05-09T00:32:03Z', 'updated_at': '2025-03-09T18:39:21Z'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "GITHUB_TOKEN =\"Your personal access token\"\n",
    "HEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "\n",
    "def get_github_user(username):\n",
    "    url = f\"https://api.github.com/users/{username}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"User not found\"}\n",
    "\n",
    "username = \"tarunsaipamulapati\"\n",
    "user_info = get_github_user(username)\n",
    "print(user_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23773e",
   "metadata": {},
   "source": [
    "Here first we are fetching all the skill data from pull requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abec10a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://api.github.com/repos/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/issues/8', 'repository_url': 'https://api.github.com/repos/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot', 'labels_url': 'https://api.github.com/repos/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/issues/8/labels{/name}', 'comments_url': 'https://api.github.com/repos/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/issues/8/comments', 'events_url': 'https://api.github.com/repos/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/issues/8/events', 'html_url': 'https://github.com/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/pull/8', 'id': 2267198779, 'node_id': 'PR_kwDOLp6NTM5t6rS8', 'number': 8, 'title': 'Technical indicator code that needs to be merged into main branch', 'user': {'login': 'tarunsaipamulapati', 'id': 132961621, 'node_id': 'U_kgDOB-zVVQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/132961621?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/tarunsaipamulapati', 'html_url': 'https://github.com/tarunsaipamulapati', 'followers_url': 'https://api.github.com/users/tarunsaipamulapati/followers', 'following_url': 'https://api.github.com/users/tarunsaipamulapati/following{/other_user}', 'gists_url': 'https://api.github.com/users/tarunsaipamulapati/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/tarunsaipamulapati/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/tarunsaipamulapati/subscriptions', 'organizations_url': 'https://api.github.com/users/tarunsaipamulapati/orgs', 'repos_url': 'https://api.github.com/users/tarunsaipamulapati/repos', 'events_url': 'https://api.github.com/users/tarunsaipamulapati/events{/privacy}', 'received_events_url': 'https://api.github.com/users/tarunsaipamulapati/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-04-27T21:41:03Z', 'updated_at': '2024-04-28T01:59:40Z', 'closed_at': '2024-04-28T01:59:37Z', 'author_association': 'COLLABORATOR', 'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0}, 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/pulls/8', 'html_url': 'https://github.com/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/pull/8', 'diff_url': 'https://github.com/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/pull/8.diff', 'patch_url': 'https://github.com/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/pull/8.patch', 'merged_at': '2024-04-28T01:59:37Z'}, 'body': 'I see that there are no conflicts, but please check before merging', 'reactions': {'url': 'https://api.github.com/repos/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/issues/8/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/ebsmartin/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/issues/8/timeline', 'performed_via_github_app': None, 'state_reason': None, 'score': 1.0}, {'url': 'https://api.github.com/repos/super30admin/DP-10/issues/189', 'repository_url': 'https://api.github.com/repos/super30admin/DP-10', 'labels_url': 'https://api.github.com/repos/super30admin/DP-10/issues/189/labels{/name}', 'comments_url': 'https://api.github.com/repos/super30admin/DP-10/issues/189/comments', 'events_url': 'https://api.github.com/repos/super30admin/DP-10/issues/189/events', 'html_url': 'https://github.com/super30admin/DP-10/pull/189', 'id': 1868278671, 'node_id': 'PR_kwDODKMNs85Y28GN', 'number': 189, 'title': 'Completed DP-10 Please review', 'user': {'login': 'tarunsaipamulapati', 'id': 132961621, 'node_id': 'U_kgDOB-zVVQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/132961621?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/tarunsaipamulapati', 'html_url': 'https://github.com/tarunsaipamulapati', 'followers_url': 'https://api.github.com/users/tarunsaipamulapati/followers', 'following_url': 'https://api.github.com/users/tarunsaipamulapati/following{/other_user}', 'gists_url': 'https://api.github.com/users/tarunsaipamulapati/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/tarunsaipamulapati/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/tarunsaipamulapati/subscriptions', 'organizations_url': 'https://api.github.com/users/tarunsaipamulapati/orgs', 'repos_url': 'https://api.github.com/users/tarunsaipamulapati/repos', 'events_url': 'https://api.github.com/users/tarunsaipamulapati/events{/privacy}', 'received_events_url': 'https://api.github.com/users/tarunsaipamulapati/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2023-08-26T21:56:48Z', 'updated_at': '2023-08-27T23:12:48Z', 'closed_at': None, 'author_association': 'NONE', 'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0}, 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/super30admin/DP-10/pulls/189', 'html_url': 'https://github.com/super30admin/DP-10/pull/189', 'diff_url': 'https://github.com/super30admin/DP-10/pull/189.diff', 'patch_url': 'https://github.com/super30admin/DP-10/pull/189.patch', 'merged_at': None}, 'body': None, 'reactions': {'url': 'https://api.github.com/repos/super30admin/DP-10/issues/189/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/super30admin/DP-10/issues/189/timeline', 'performed_via_github_app': None, 'state_reason': None, 'score': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "def get_user_pull_requests(username):\n",
    "    url = f\"https://api.github.com/search/issues?q=author:{username}+type:pr\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"items\"]\n",
    "    else:\n",
    "        return {\"error\": \"No pull requests found\"}\n",
    "\n",
    "pull_requests = get_user_pull_requests(username)\n",
    "print(pull_requests[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93bbaa",
   "metadata": {},
   "source": [
    "From the pull requests we are trying to extract all the skills information using predefined skill_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d016a13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tarun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "SKILL_SET = { \"Python\", \"JavaScript\", \"Java\", \"C#\", \"C++\", \"SQL\", \"R\", \"TypeScript\", \"Go\", \"Ruby\",\n",
    "    \"Shell Scripting (Bash)\", \"Kotlin\", \"Swift\", \"PHP\", \"HTML5\", \"CSS3\", \"React.js\", \"Vue.js\", \"Angular\", \"Node.js\", \"Express.js\",\n",
    "    \"Django\", \"Flask\", \"ASP.NET\", \"NumPy\", \"Pandas\", \"Matplotlib\", \"Seaborn\", \"Plotly\", \"scikit-learn\",\n",
    "    \"TensorFlow\", \"PyTorch\", \"Jupyter Notebook\", \"XGBoost\", \"LightGBM\", \"MySQL\", \"PostgreSQL\", \"MongoDB\", \"Cassandra\", \"SQLite\", \"Snowflake\", \"BigQuery\",\n",
    "    \"Hadoop\", \"Spark\", \"Apache NiFi\", \"Talend\", \"Azure Data Factory\", \"AWS\", \"Azure\", \"Google Cloud Platform (GCP)\", \"Docker\", \"Kubernetes\",\n",
    "    \"Git\", \"GitHub\", \"GitLab\", \"Terraform\", \"Jenkins\", \"GitHub Actions\", \"CircleCI\", \"Power BI\", \"Tableau\", \"Excel (Advanced)\", \"Looker\", \"Metabase\",\n",
    "    \"React Native\", \"Flutter\", \"Swift (iOS)\", \"Kotlin (Android)\"\n",
    "}\n",
    "\n",
    "\n",
    "def extract_skills_from_text(text):\n",
    "    if not text:\n",
    "        return set()\n",
    "\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    return set(words) & SKILL_SET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60a1b819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills from PRs: set()\n"
     ]
    }
   ],
   "source": [
    "def extract_skills_from_prs(username):\n",
    "    pull_requests = get_user_pull_requests(username)\n",
    "    \n",
    "    skills = set()\n",
    "    if isinstance(pull_requests, list):\n",
    "        for pr in pull_requests:\n",
    "            pr_title = pr.get(\"title\", \"\")\n",
    "            pr_body = pr.get(\"body\", \"\")\n",
    "            skills.update(extract_skills_from_text(pr_title))\n",
    "            skills.update(extract_skills_from_text(pr_body))\n",
    "    \n",
    "    return skills\n",
    "\n",
    "user_skills = extract_skills_from_prs(username)\n",
    "print(f\"Extracted Skills from PRs: {user_skills}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3560b3",
   "metadata": {},
   "source": [
    "In the below cell, we are trying to check if there are any files present in the pull requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e4fce9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_files(owner, repo, pr_number):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/files\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"PR files not found\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffac92",
   "metadata": {},
   "source": [
    "Here, we are trying to extrct all the information using the file extensions that were uploaded by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7eed5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programming Languages from PRs: {'Python'}\n"
     ]
    }
   ],
   "source": [
    "def extract_languages_from_pr(username):\n",
    "    pull_requests = get_user_pull_requests(username)\n",
    "\n",
    "    EXTENSION_MAPPING = {\n",
    "        \".py\": \"Python\", \".js\": \"JavaScript\", \".java\": \"Java\",\n",
    "        \".cpp\": \"C++\", \".cs\": \"C#\", \".ts\": \"TypeScript\",\n",
    "        \".rb\": \"Ruby\", \".go\": \"Go\", \".rs\": \"Rust\"\n",
    "    }\n",
    "\n",
    "    languages = set()\n",
    "    if isinstance(pull_requests, list):\n",
    "        for pr in pull_requests:\n",
    "            repo_url = pr.get(\"repository_url\", \"\")\n",
    "            pr_number = pr.get(\"number\", 0)\n",
    "            repo_owner, repo_name = repo_url.replace(\"https://api.github.com/repos/\", \"\").split(\"/\")\n",
    "            \n",
    "            files = get_pr_files(repo_owner, repo_name, pr_number)\n",
    "            if isinstance(files, list):\n",
    "                for file in files:\n",
    "                    for ext, lang in EXTENSION_MAPPING.items():\n",
    "                        if file[\"filename\"].endswith(ext):\n",
    "                            languages.add(lang)\n",
    "\n",
    "    return languages\n",
    "\n",
    "user_languages = extract_languages_from_pr(username)\n",
    "print(f\"Programming Languages from PRs: {user_languages}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17cc26",
   "metadata": {},
   "source": [
    "Here we are trying to extract skill information from repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d0b8201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repositories owned by tarunsaipamulapati: ['Array-1', 'Array-2', 'Array-3', 'Array-4', 'Backtracking-2', 'Backtracking-3', 'Bactracking-1', 'BFS-1', 'BFS-2', 'BFS-2.1', 'BFS-3', 'BFS-4', 'Binary-Search-1', 'Binary-Search-2', 'Binary-Search-3', 'Binary-Search-3.1', 'Binary-Search-4', 'Competitive-Coding-1', 'Competitive-Coding-10', 'Competitive-Coding-11', 'Competitive-Coding-2', 'Competitive-Coding-4', 'Competitive-Coding-5', 'Competitive-Coding-7', 'Competitive-Coding-8', 'Competitive-Coding-9', 'Competitive_Coding-3', 'Design-1', 'Design-2', 'Design-4', 'Design-5', 'Design-6', 'DFS-1', 'DFS-2', 'DP-1', 'DP-10', 'DP-2', 'DP-4', 'DP-5', 'DP-7', 'DP-8', 'DP-9', 'Graph-1', 'Greedy-1', 'Greedy-2', 'Hashing-1', 'Hashing-2', 'Heaps-1', 'Linked-List-1', 'Linked-List-2', 'PreCourse_1', 'PreCourse_2', 'Stack-1', 'Stack-2', 'STEDI_Human_Balance_Analytics', 'Strings-1', 'Strings-2', 'Strings-3', 'Trees-1', 'Trees-2', 'Trees-3', 'Trees-4', 'Trees-5', 'Tries-1', 'Two-Pointers-1', 'Two-Pointers-2', 'Udacity_final_project']\n"
     ]
    }
   ],
   "source": [
    "def get_user_repos(username):\n",
    "    repos = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        url = f\"https://api.github.com/users/{username}/repos?per_page=100&page={page}\"\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "        if response.status_code != 200 or not response.json():\n",
    "            break\n",
    "\n",
    "        repos.extend(response.json())\n",
    "        page += 1\n",
    "\n",
    "    return [repo[\"name\"] for repo in repos]\n",
    "user_repos = get_user_repos(username)\n",
    "print(f\"Repositories owned by {username}: {user_repos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "08eb8829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_files(owner, repo):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/git/trees/main?recursive=1\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        tree = response.json().get(\"tree\", [])\n",
    "        return [file[\"path\"] for file in tree if file[\"type\"] == \"blob\"]\n",
    "    else:\n",
    "        return {\"error\": f\"Files not found for {repo}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "85179f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Files from All Repositories of tarunsaipamulapati:\n",
      "\n",
      "ðŸ“‚ Array-1: 1 files\n",
      "\n",
      "ðŸ“‚ Array-2: 1 files\n",
      "\n",
      "ðŸ“‚ Array-3: 1 files\n",
      "\n",
      "ðŸ“‚ Array-4: 1 files\n",
      "\n",
      "ðŸ“‚ Backtracking-2: 1 files\n",
      "\n",
      "ðŸ“‚ Backtracking-3: 1 files\n",
      "\n",
      "ðŸ“‚ Bactracking-1: 1 files\n",
      "\n",
      "ðŸ“‚ BFS-1: 1 files\n",
      "\n",
      "ðŸ“‚ BFS-2: 1 files\n",
      "\n",
      "ðŸ“‚ BFS-2.1: 3 files\n",
      "\n",
      "ðŸ“‚ BFS-3: 1 files\n",
      "\n",
      "ðŸ“‚ BFS-4: 1 files\n",
      "\n",
      "ðŸ“‚ Binary-Search-1: 1 files\n",
      "\n",
      "ðŸ“‚ Binary-Search-2: 1 files\n",
      "\n",
      "ðŸ“‚ Binary-Search-3: 1 files\n",
      "\n",
      "ðŸ“‚ Binary-Search-3.1: 3 files\n",
      "\n",
      "ðŸ“‚ Binary-Search-4: 1 files\n",
      "\n",
      "ðŸ“‚ Competitive-Coding-1: 1 files\n",
      "\n",
      "ðŸ“‚ Competitive-Coding-10: 1 files\n",
      "\n",
      "ðŸ“‚ Competitive-Coding-11: 1 files\n",
      "\n",
      "ðŸ“‚ Competitive-Coding-2: 1 files\n",
      "\n",
      "ðŸ“‚ Competitive-Coding-4: 1 files\n",
      "\n",
      "ðŸ“‚ Competitive-Coding-5: 1 files\n",
      "\n",
      "ðŸ“‚ Competitive-Coding-7: 1 files\n",
      "\n",
      "ðŸ“‚ Competitive-Coding-8: 1 files\n",
      "\n",
      "ðŸ“‚ Competitive-Coding-9: 1 files\n",
      "\n",
      "ðŸ“‚ Competitive_Coding-3: 1 files\n",
      "\n",
      "ðŸ“‚ Design-1: 1 files\n",
      "\n",
      "ðŸ“‚ Design-2: 1 files\n",
      "\n",
      "ðŸ“‚ Design-4: 1 files\n",
      "\n",
      "ðŸ“‚ Design-5: 1 files\n",
      "\n",
      "ðŸ“‚ Design-6: 1 files\n",
      "\n",
      "ðŸ“‚ DFS-1: 1 files\n",
      "\n",
      "ðŸ“‚ DFS-2: 1 files\n",
      "\n",
      "ðŸ“‚ DP-1: 1 files\n",
      "\n",
      "ðŸ“‚ DP-10: 1 files\n",
      "\n",
      "ðŸ“‚ DP-2: 1 files\n",
      "\n",
      "ðŸ“‚ DP-4: 1 files\n",
      "\n",
      "ðŸ“‚ DP-5: 1 files\n",
      "\n",
      "ðŸ“‚ DP-7: 1 files\n",
      "\n",
      "ðŸ“‚ DP-8: 1 files\n",
      "\n",
      "ðŸ“‚ DP-9: 1 files\n",
      "\n",
      "ðŸ“‚ Graph-1: 1 files\n",
      "\n",
      "ðŸ“‚ Greedy-1: 1 files\n",
      "\n",
      "ðŸ“‚ Greedy-2: 1 files\n",
      "\n",
      "ðŸ“‚ Hashing-1: 1 files\n",
      "\n",
      "ðŸ“‚ Hashing-2: 1 files\n",
      "\n",
      "ðŸ“‚ Heaps-1: 1 files\n",
      "\n",
      "ðŸ“‚ Linked-List-1: 1 files\n",
      "\n",
      "ðŸ“‚ Linked-List-2: 1 files\n",
      "\n",
      "ðŸ“‚ PreCourse_1: 1 files\n",
      "\n",
      "ðŸ“‚ PreCourse_2: 1 files\n",
      "\n",
      "ðŸ“‚ Stack-1: 1 files\n",
      "\n",
      "ðŸ“‚ Stack-2: 1 files\n",
      "\n",
      "ðŸ“‚ STEDI_Human_Balance_Analytics: 31 files\n",
      "\n",
      "ðŸ“‚ Strings-1: 1 files\n",
      "\n",
      "ðŸ“‚ Strings-2: 1 files\n",
      "\n",
      "ðŸ“‚ Strings-3: 1 files\n",
      "\n",
      "ðŸ“‚ Trees-1: 1 files\n",
      "\n",
      "ðŸ“‚ Trees-2: 1 files\n",
      "\n",
      "ðŸ“‚ Trees-3: 1 files\n",
      "\n",
      "ðŸ“‚ Trees-4: 1 files\n",
      "\n",
      "ðŸ“‚ Trees-5: 1 files\n",
      "\n",
      "ðŸ“‚ Tries-1: 1 files\n",
      "\n",
      "ðŸ“‚ Two-Pointers-1: 1 files\n",
      "\n",
      "ðŸ“‚ Two-Pointers-2: 1 files\n",
      "\n",
      "ðŸ“‚ Udacity_final_project: 28 files\n"
     ]
    }
   ],
   "source": [
    "def get_all_files_from_user(username):\n",
    "    all_files = {}\n",
    "    repos = get_user_repos(username)\n",
    "\n",
    "    for repo in repos:\n",
    "        repo_files = get_repo_files(username, repo)\n",
    "        all_files[repo] = repo_files\n",
    "\n",
    "    return all_files\n",
    "all_repo_files = get_all_files_from_user(username)\n",
    "print(f\"All Files from All Repositories of {username}:\")\n",
    "for repo, files in all_repo_files.items():\n",
    "    print(f\"\\nðŸ“‚ {repo}: {len(files)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44085404",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSION_TO_SKILLS = {\n",
    "    \".py\": \"Python\",\n",
    "    \".js\": \"JavaScript\",\n",
    "    \".java\": \"Java\",\n",
    "    \".cpp\": \"C++\",\n",
    "    \".cs\": \"C#\",\n",
    "    \".ts\": \"TypeScript\",\n",
    "    \".rb\": \"Ruby\",\n",
    "    \".go\": \"Go\",\n",
    "    \".rs\": \"Rust\",\n",
    "    \".sql\": \"SQL\",\n",
    "    \".html\": \"HTML\",\n",
    "    \".css\": \"CSS\",\n",
    "    \".sh\": \"Shell Scripting\",\n",
    "    \".json\": \"JSON\",\n",
    "    \".xml\": \"XML\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2152f760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Extracted Skills from tarunsaipamulapati's Repositories: {'JSON', 'SQL', 'Python'}\n"
     ]
    }
   ],
   "source": [
    "def extract_skills_from_all_files(username):\n",
    "    all_files = get_all_files_from_user(username)\n",
    "    \n",
    "    skills = set()\n",
    "    for repo, files in all_files.items():\n",
    "        for file in files:\n",
    "            for ext, skill in EXTENSION_TO_SKILLS.items():\n",
    "                if file.endswith(ext):\n",
    "                    skills.add(skill)\n",
    "\n",
    "    return skills\n",
    "\n",
    "user_skills_repos = extract_skills_from_all_files(username)\n",
    "print(f\"\\n Extracted Skills from {username}'s Repositories: {user_skills_repos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9cb555ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GitHub Username': 'tarunsaipamulapati', 'Full Name': None, 'Company': None, 'Bio': None, 'Public Repos': 67, 'Followers': 1, 'Skills from PRs': [], 'Modified File types from PRs': ['Python'], \"Skills from all repo's\": ['JSON', 'SQL', 'Python']}\n"
     ]
    }
   ],
   "source": [
    "def get_user_profile_with_skills(username):\n",
    "    user_info = get_github_user(username)\n",
    "    user_skills = extract_skills_from_prs(username)\n",
    "    user_languages = extract_languages_from_pr(username)\n",
    "\n",
    "    return {\n",
    "        \"GitHub Username\": user_info.get(\"login\"),\n",
    "        \"Full Name\": user_info.get(\"name\"),\n",
    "        \"Company\": user_info.get(\"company\"),\n",
    "        \"Bio\": user_info.get(\"bio\"),\n",
    "        \"Public Repos\": user_info.get(\"public_repos\"),\n",
    "        \"Followers\": user_info.get(\"followers\"),\n",
    "        \"Skills from PRs\": list(user_skills),\n",
    "        \"Modified File types from PRs\": list(user_languages),\n",
    "        \"Skills from all repo's\": list(user_skills_repos)\n",
    "    }\n",
    "profile_data = get_user_profile_with_skills(username)\n",
    "print(profile_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9a6cb",
   "metadata": {},
   "source": [
    "After collecting all the information from various places in the GitHUb, we are combining them into a final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aaf6aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list=set().union(user_skills, user_languages, user_skills_repos, temporary_skill_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5d54620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list=list(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1f9bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_skills_final_data[name]=final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba807c4",
   "metadata": {},
   "source": [
    "We are storing all the information in a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aab18d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person1': ['Maven', 'Waterfall', 'Python', 'Spring (IOC', 'Apache Tomcat', 'REST', 'OpenAPI', 'Java', 'Kubernetes', 'AWS (EC2', 'Postman', 'HTML', 'IAM', 'System Design', 'RDS', 'MongoDB', 'Cognito User Pools', 'Microservice using Spring Cloud', 'Fargate)', 'Distributed Systems', 'XML', 'AOP', 'Gradle', 'Swagger', 'ECS', 'MVC', 'MySQL', 'Mockito', 'Karate BDD', 'CloudWatch', 'JavaScript', 'SQL', 'JMS)', 'S3', 'Bitbucket', 'Jira', 'Git', 'SonarQube', 'Spring Boot', 'CSS', 'Docker', 'SOAP', 'Event-Driven Architecture', 'Agile', 'JUnit', 'GitHub', 'Jenkins', 'CI/CD', 'SVN', 'Data Structures and Algorithms', 'JDBC', 'Microservices', 'NoSQL'], 'person2': ['MATLAB', 'Python', 'Kalman Filter', 'SQL', 'Tableau', 'TensorFlow', 'CenterTrack', 'Git', 'nltk', 'Kubernetes', 'Java', 'OpenCL', 'Onnx', 'Amazon SageMaker', 'ggplot', 'C++', 'Docker', 'Scikit-learn', 'PIL', 'Keras', 'dlib', 'React JS', 'OpenCV', 'XML', 'PyTorch', 'Amazon EC', 'Matplotlib', 'Tflite', 'Here are the extracted skills from the resume:', 'Hexagon', 'YOLOv'], 'person3': ['JSON', 'Python', 'JavaScript', 'SQL', 'Tableau', 'Event Hubs', 'Oracle', 'SQL Server', 'Java', 'Cassandra', 'C++', 'CSS', 'Apache Spark', 'HTML', 'AWS (Glue', 'MongoDB', 'Feature Engineering', 'Power BI', 'Azure Synapse', 'Stream Processing', 'Machine Learning', 'matplotlib', 'PostgreSQL', 'S3)', 'Seaborn', 'Bootstrap', 'Hadoop HDFS', 'Microsoft Excel', 'Here are the extracted skills from the resume:', 'Redshift', 'R', 'Apache Airflow', 'Athena', 'MySQL', 'C', 'Data Warehousing', 'PHP']}\n"
     ]
    }
   ],
   "source": [
    "print(user_skills_final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad3c30",
   "metadata": {},
   "source": [
    "# GitHub open task1 details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37aa310",
   "metadata": {},
   "source": [
    "Here we are collecting information from GitHub tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09895357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Python Contribution\n",
      "Labels: []\n",
      "Body:\n",
      "  Incorrect Output in calculate_total Function\r\n",
      "\r\n",
      "Description:I have encountered an issue in the calculate_total() function in the billing.py file. When I provide input with a list of prices, the total calculated is incorrect. The function does not seem to handle decimal values correctly.\r\n",
      "```python\r\n",
      "from billing import calculate_total\r\n",
      "prices = [12.99, 5.49, 8.75]\r\n",
      "total = calculate_total(prices)\r\n",
      "print(total)\r\n",
      "``` \r\n",
      "\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = f\"https://api.github.com/repos/geekcomputers/Python/issues/2492\"\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/vnd.github+json\"\n",
    "    }\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    issue = response.json()\n",
    "    print(\"Title:\", issue.get(\"title\"))\n",
    "    print(\"Labels:\", [label[\"name\"] for label in issue.get(\"labels\", [])])\n",
    "    print(\"Body:\\n\", issue.get(\"body\"))\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n",
    "title=issue.get(\"title\")\n",
    "labels=[label[\"name\"] for label in issue.get(\"labels\", [])]\n",
    "body=issue.get(\"body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "04b810ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected Comments:\n",
      "\n",
      "--- Comment #1 by hazey21 at 2025-01-18T21:29:14Z ---\n",
      "The problem has occurred because the calculate_total function  is not properly handling the decimal values. You need to insert the Decimal class to avoid these errors. \n",
      "\n",
      " ```\n",
      " from decimal import Decimal\n",
      "\n",
      "def calculate_total(prices):\n",
      "    total= sum(Decimal(str(price)) for price in prices)\n",
      "    return total \n",
      "``` \n",
      " ``` \n",
      "from billing import calculate_total\n",
      "\n",
      "prices = [12.99, 5.49, 8.75]\n",
      "total = calculate_total(prices)\n",
      "print(total)\n",
      " ``` \n",
      "\n",
      "--- Comment #2 by zilolaegamberganova at 2025-01-19T12:20:17Z ---\n",
      "Hello, Iâ€™m Zilola. Iâ€™ve been looking into the issue regarding the `calculate_total` function.\n",
      "\n",
      "**Problem**: As you mentioned, it seems that the function is not properly handling decimal values when calculating the total.\n",
      "\n",
      "**Suggestion**: I believe the issue can be resolved by using the `Decimal` class for accurate calculations. Additionally, I recommend using the `round()` function to ensure that the values are rounded correctly.\n",
      "\n",
      "If you need further assistance, I am here to help. Thank you!\n",
      "\n",
      "--- Comment #3 by zilolaegamberganova at 2025-01-19T12:25:18Z ---\n",
      "Hello, Iâ€™m Zilola. I need help with my code because it has an error.\n",
      "\n",
      "I have a simple chatbot implementation, but I am facing an issue. Hereâ€™s the code:\n",
      "\n",
      "```python\n",
      "class ChatBot:\n",
      "    def __init__(self, name):\n",
      "        self.name = name\n",
      "\n",
      "    def greet(self):\n",
      "        return \"Hello! I'm \" + self.name\n",
      "\n",
      "    def respond(self, message):\n",
      "        if message.lower() == \"how are you?\":\n",
      "            return \"I'm fine, thank you!\"\n",
      "        elif message.lower() == \"what's your name?\":\n",
      "            return \"My name is \" + self.name\n",
      "        else:\n",
      "            return \"Sorry, I don't understand.\"\n",
      "\n",
      "bot = ChatBot(None)\n",
      "print(bot.greet())\n",
      "print(bot.respond(\"How are you?\"))\n",
      "\n",
      "--- Comment #4 by navjotmaan at 2025-01-20T07:02:37Z ---\n",
      "you are getting issue because of None, you can solve it by using a default name. Change self.name as given below. I hope it helps.\n",
      "\n",
      "\n",
      "\n",
      "class ChatBot:\n",
      "    def __init__(self, name):\n",
      "        self.name = name\n",
      "\n",
      "    def greet(self):\n",
      "        # Handle None by using a default name\n",
      "        return \"Hello! I'm \"  + (self.name if self.name else \"ChatBOT\")\n",
      "\n",
      "    def respond(self, message):\n",
      "        if message.lower() == \"how are you?\":\n",
      "            return \"I'm fine, thank you!\"\n",
      "        elif message.lower() == \"what's your name?\":\n",
      "            return \"My name is \" + (self.name if self.name else \"ChatBOT\")\n",
      "        else:\n",
      "            return \"Sorry, I don't understand.\"\n",
      "\n",
      "# Create a Chatbot instance with None as the name\n",
      "bot = ChatBot(None)\n",
      "print(bot.greet())\n",
      "print(bot.respond(\"How are you?\"))\n"
     ]
    }
   ],
   "source": [
    "comments_url = issue.get(\"comments_url\")\n",
    "comments_response = requests.get(comments_url, headers=headers)\n",
    "\n",
    "all_comments = []\n",
    "if comments_response.status_code == 200:\n",
    "    comments = comments_response.json()\n",
    "    for comment in comments:\n",
    "        comment_data = {\n",
    "            \"author\": comment[\"user\"][\"login\"],\n",
    "            \"created_at\": comment[\"created_at\"],\n",
    "            \"body\": comment[\"body\"]\n",
    "        }\n",
    "        all_comments.append(comment_data)\n",
    "    print(\"\\nCollected Comments:\")\n",
    "    for i, c in enumerate(all_comments, 1):\n",
    "        print(f\"\\n Comment #{i} by {c['author']} at {c['created_at']}\")\n",
    "        print(c[\"body\"])\n",
    "else:\n",
    "    print(f\"\\nError getting comments: {comments_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e8927a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the GitHub issue details provided, the programming skills required to address and resolve the issue with the `calculate_total` function are as follows:\n",
      "\n",
      "1. **Python Programming**: Strong understanding of Python syntax and semantics to read, interpret, and modify Python code properly.\n",
      "\n",
      "2. **Decimal Module Usage**: Knowledge of Python's `decimal` module for precise arithmetic operations, especially when dealing with floating-point numbers and handling decimal values accurately.\n",
      "\n",
      "3. **Debugging Skills**: Ability to identify and understand the problem with the current function implementation, such as why floating-point arithmetic could cause inaccuracies in total calculation.\n",
      "\n",
      "4. **Code Refactoring**: Skill in refactoring existing code to improve or fix issues without altering the intended functionality, such as replacing float logic with `Decimal`.\n",
      "\n",
      "5. **Unit Testing**: Competency in writing test cases to ensure that the revised function performs correctly and handles various input cases effectively post-modification.\n",
      "\n",
      "These skills should enable a developer to analyze the issue with the current implementation and apply a suitable fix using the `Decimal` class for accurate total calculation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=api_key_)\n",
    "question= f\"I have details of a GitHub issue in {title}, {labels},{all_comments}, and {body}. Please give me a final list of only programming skills that are required to complete the issue\"\n",
    "message = [\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages = message\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2d9639fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49ef60d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Based on the previous output, the list of skill names would be:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Python Programming\",\n",
      "    \"Decimal Module Usage\",\n",
      "    \"Debugging Skills\",\n",
      "    \"Code Refactoring\",\n",
      "    \"Unit Testing\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=api_key_)\n",
    "question= f\"Based on the previous output, I want you to extract only name of the skills in form of a python list. It should be like ['list1', 'list2',....]\"\n",
    "message = [\n",
    "    {\"role\": \"assistant\", \"content\": output},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages = message\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "55aec060",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills=[\"Python Programming\", \"Decimal Module Usage\", \"Debugging Skills\", \"Code Refactoring\", \"Unit Testing\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a92b2",
   "metadata": {},
   "source": [
    "# Mapping people to open task and providing a bootcamp for missing skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e360cb",
   "metadata": {},
   "source": [
    "Here, I after collecting task information from the GitHub and skill information from resume & GtHub, we are mapping open task to the user skillset using LLM. Then LLM picks the best person to solve the issue with proper justification. Moreover, it provides a bootcamp in order to master to master required skills to better solve the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9de8ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To address the task that requires the skills of **Python Programming, Decimal Module Usage, Debugging Skills, Code Refactoring, and Unit Testing**, we can analyze the skills of each user:\n",
      "\n",
      "1. **Person1**: Has Python and a strong background in various programming-related skills but lacks specific experience with the Decimal module, debugging techniques, and unit testing practices.\n",
      "  \n",
      "2. **Person2**: Also has Python and a mix of data science and software development skills. However, there is no clear indication of experience in debugging or unit testing specifically for Python.\n",
      "\n",
      "3. **Person3**: Has Python skills and various data-handling and data visualizing tools but also lacks explicit skills related to debugging and unit testing for Python.\n",
      "\n",
      "Among the users, **Person1** is the most suitable candidate due to their extensive software development background, particularly with Python and related technologies.\n",
      "\n",
      "### Skills to Master for Effective Task Completion:\n",
      "1. **Decimal Module Usage**: Understanding how to use Python's `decimal` module for precise decimal arithmetic.\n",
      "2. **Debugging Skills**: Proficiency in debugging Python code, utilizing tools/resources like `pdb`, `PyCharm`, or `VSCode` debugging features.\n",
      "3. **Code Refactoring**: Learning techniques for cleaning up and structuring existing code for better performance and readability.\n",
      "4. **Unit Testing**: Familiarity with Python's `unittest` module or other frameworks like `pytest` to write unit tests.\n",
      "\n",
      "### Bootcamp Outline to Master Required Skills:\n",
      "\n",
      "#### Bootcamp Duration: 4 weeks\n",
      "\n",
      "### Week 1: Python Programming Fundamentals\n",
      "- **Day 1-3**: Refresh Python Basics (Data types, control flow, functions)\n",
      "- **Day 4-5**: Advanced Python Topics (list comprehensions, decorators, context managers)\n",
      "\n",
      "### Week 2: Mastering the Decimal Module and Debugging\n",
      "- **Day 6-7**: Introduction to the Decimal Module\n",
      "  - Use cases\n",
      "  - Working with precision\n",
      "\n",
      "- **Day 8-10**: Debugging Techniques\n",
      "  - Understand the debugging process\n",
      "  - Using `pdb`, logging for debugging, and integrated IDE debugging tools\n",
      "\n",
      "### Week 3: Code Refactoring Techniques\n",
      "- **Day 11-12**: Principles of Clean Code\n",
      "  - Understanding code smells\n",
      "  - Identifying areas for refactoring\n",
      "  \n",
      "- **Day 13-15**: Refactoring Practices\n",
      "  - Practical examples and exercises\n",
      "  - Refactoring legacy code\n",
      "\n",
      "### Week 4: Unit Testing\n",
      "- **Day 16-17**: Introduction to Unit Testing in Python\n",
      "  - Overview of testing frameworks: `unittest`, `pytest`\n",
      "  \n",
      "- **Day 18-20**: Writing and Running Tests\n",
      "  - Test-driven development (TDD)\n",
      "  - Writing effective test cases and assertions\n",
      "\n",
      "### Additional Resources:\n",
      "- Books: \"Clean Code\" by Robert C. Martin, \"Automate the Boring Stuff with Python\" by Al Sweigart\n",
      "- Online courses and platforms (Coursera, Udemy, freeCodeCamp)\n",
      "- Access to coding challenge platforms (LeetCode, HackerRank) for practice\n",
      "\n",
      "### Assessment:\n",
      "- Weekly coding challenges\n",
      "- Final project that incorporates the implementation of the Decimal module, a refactored codebase, and a suite of unit tests.\n",
      "\n",
      "This bootcamp will ensure that Person1 and anyone else participating gains proficiency in the required skills to effectively tackle the task at hand.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=api_key_)\n",
    "question= f\"Based on the users and their skills available in {user_skills_final_data}, to which user do you assign above issue that needs {skills}? In that dictionary, key is the name of the user.\"\n",
    "question1=f\"If you choose a user, what are the other skills required to master by user to complete that task effectively\"\n",
    "question2=f\"Provide a bootcamp to master those skills\"\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question1\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question2\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2a8a4",
   "metadata": {},
   "source": [
    "# GitHub open task2 details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ed93c522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Cannot recoginze Sql identifer b.XXX\n",
      "Labels: []\n",
      "Body:\n",
      " ```\r\n",
      "SELECT\r\n",
      "  *\r\n",
      "FROM\r\n",
      "  `table1` as a,\r\n",
      "  `table2` as b\r\n",
      "WHERE\r\n",
      "a.XXX= b.XXX\r\n",
      "```\r\n",
      "\r\n",
      "find ERROR\r\n",
      "\r\n",
      "`{\"reason\":\"There was internal problem at backend\",\"details\":\"Cannot recoginze Sql identifer b.XXX\",\"type\":\"SqlParseException\"}\r\n",
      "`\r\n",
      "```\r\n",
      "\r\n",
      "SELECT a.XXX FROM `table1` as a limit 1   [is OK]\r\n",
      "SELECT b.XXX FROM `table2` as b limit 1   [is OK]\r\n",
      "```\r\n",
      "\r\n",
      "opendistroforelasticsearch log\r\n",
      "```\r\n",
      "\r\n",
      "com.amazon.opendistroforelasticsearch.sql.legacy.exception.SqlParseException: Cannot recoginze Sql identifer b.XXX\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.query.maker.Maker.make(Maker.java:204) ~[legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.query.maker.Maker.make(Maker.java:127) ~[legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.query.maker.QueryMaker.explanWhere(QueryMaker.java:59) ~[legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.query.maker.QueryMaker.explanWhere(QueryMaker.java:64) ~[legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.query.maker.QueryMaker.explain(QueryMaker.java:46) ~[legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.query.DefaultQueryAction.setWhere(DefaultQueryAction.java:223) ~[legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.query.DefaultQueryAction.buildRequest(DefaultQueryAction.java:97) ~[legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.query.DefaultQueryAction.explain(DefaultQueryAction.java:88) ~[legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.executor.format.PrettyFormatRestExecutor.buildProtocolForDefaultQuery(PrettyFormatRestExecutor.java:102) ~[legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.executor.format.PrettyFormatRestExecutor.execute(PrettyFormatRestExecutor.java:77) [legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.executor.format.PrettyFormatRestExecutor.execute(PrettyFormatRestExecutor.java:53) [legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.executor.AsyncRestExecutor.doExecuteWithTimeMeasured(AsyncRestExecutor.java:161) [legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.executor.AsyncRestExecutor.lambda$async$1(AsyncRestExecutor.java:121) [legacy-1.8.0.0.jar:?]\r\n",
      "        at com.amazon.opendistroforelasticsearch.sql.legacy.utils.LogUtils.lambda$withCurrentContext$0(LogUtils.java:72) [legacy-1.8.0.0.jar:?]\r\n",
      "        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:684) [elasticsearch-7.10.2.jar:7.10.2]\r\n",
      "        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]\r\n",
      "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]\r\n",
      "        at java.lang.Thread.run(Thread.java:832) [?:?]\r\n",
      "\r\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "url = f\"https://api.github.com/repos/opendistro-for-elasticsearch/sql/issues/1169\"\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/vnd.github+json\"\n",
    "    }\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    issue = response.json()\n",
    "    print(\"Title:\", issue.get(\"title\"))\n",
    "    print(\"Labels:\", [label[\"name\"] for label in issue.get(\"labels\", [])])\n",
    "    print(\"Body:\\n\", issue.get(\"body\"))\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n",
    "title=issue.get(\"title\")\n",
    "labels=[label[\"name\"] for label in issue.get(\"labels\", [])]\n",
    "body=issue.get(\"body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f8212598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected Comments:\n"
     ]
    }
   ],
   "source": [
    "comments_url = issue.get(\"comments_url\")\n",
    "comments_response = requests.get(comments_url, headers=headers)\n",
    "\n",
    "all_comments = []\n",
    "if comments_response.status_code == 200:\n",
    "    comments = comments_response.json()\n",
    "    for comment in comments:\n",
    "        comment_data = {\n",
    "            \"author\": comment[\"user\"][\"login\"],\n",
    "            \"created_at\": comment[\"created_at\"],\n",
    "            \"body\": comment[\"body\"]\n",
    "        }\n",
    "        all_comments.append(comment_data)\n",
    "    print(\"\\nCollected Comments:\")\n",
    "    for i, c in enumerate(all_comments, 1):\n",
    "        print(f\"\\n Comment #{i} by {c['author']} at {c['created_at']} \")\n",
    "        print(c[\"body\"])\n",
    "else:\n",
    "    print(f\"\\nError getting comments: {comments_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "27b43726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To resolve the issue with the SQL query involving Open Distro for Elasticsearch, you'll need to have the following programming skills:\n",
      "\n",
      "1. **SQL Knowledge**: Understanding SQL syntax, especially involving joins and aliases, is essential for identifying and correcting the issue.\n",
      "\n",
      "2. **Elasticsearch**: Familiarity with Elasticsearch, particularly with how SQL queries are translated into Elasticsearch queries within Open Distro for Elasticsearch.\n",
      "\n",
      "3. **Debugging**: Skill in troubleshooting and debugging errors based on logs and error messages to understand what went wrong.\n",
      "\n",
      "4. **Java Programming**: Knowledge of Java can be beneficial, especially when dealing with Elasticsearch plugins written in Java, like the one mentioned (legacy-1.8.0.0.jar).\n",
      "\n",
      "5. **Error Handling**: Understanding and managing exceptions in a software context, translating error messages into actionable tasks.\n",
      "\n",
      "These skills collectively handle identifying the root cause of the issue and effectively implementing a solution.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=api_key_)\n",
    "question= f\"I have details of a GitHub issue in {title}, {labels},{all_comments}, and {body}. Please give me a final list of only programming skills that are required to complete the issue\"\n",
    "message = [\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages = message\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b18a6022",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3522a3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Based on the previous output, the list of skills extracted would be:\n",
      "\n",
      "```python\n",
      "['SQL Knowledge', 'Elasticsearch', 'Debugging', 'Java Programming', 'Error Handling']\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=api_key_)\n",
    "question= f\"Based on the previous output, I want you to extract only name of the skills in form of a python list. It should be like ['list1', 'list2',....]\"\n",
    "message = [\n",
    "    {\"role\": \"assistant\", \"content\": output},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages = message\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "df32406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills=['SQL Knowledge', 'Elasticsearch', 'Debugging', 'Java Programming', 'Error Handling']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07678e62",
   "metadata": {},
   "source": [
    "# Mapping people to open task and providing a bootcamp for missing skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe410a7",
   "metadata": {},
   "source": [
    "Here, I after collecting task information from the GitHub and skill information from resume & GtHub, we are mapping open task to the user skillset using LLM. Then LLM picks the best person to solve the issue with proper justification. Moreover, it provides a bootcamp in order to master to master required skills to better solve the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b8ab4312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To effectively address an issue that requires `['SQL Knowledge', 'Elasticsearch', 'Debugging', 'Java Programming', 'Error Handling']`, I would assign the task to **person1**. This user already possesses a strong foundation in SQL and Java programming, alongside various other relevant skills. While person2 and person3 have robust data science and programming skills, person1's background aligns more closely with the task requirements.\n",
      "\n",
      "### Additional Skills Required for Mastering the Task\n",
      "To complete the task effectively, person1 might need to further master the following skills:\n",
      "1. **Elasticsearch:**\n",
      "   - Understand how to set up and manage an Elasticsearch cluster.\n",
      "   - Learn how to index, search, and analyze data in Elasticsearch.\n",
      "\n",
      "2. **Debugging:**\n",
      "   - Gain proficiency in debugging Java applications, understanding common bugs, and utilizing debugging tools effectively.\n",
      "   - Learn about common debugging patterns and practices in distributed systems.\n",
      "\n",
      "3. **Error Handling:**\n",
      "   - Understand patterns and practices for handling errors gracefully in Java applications.\n",
      "   - Learn about logging frameworks and best practices for capturing and analyzing errors.\n",
      "\n",
      "### Bootcamp Outline to Master Required Skills\n",
      "\n",
      "**Duration: 6-8 Weeks**\n",
      "**Format: Combination of Online and Hands-on Labs**\n",
      "\n",
      "#### Week 1-2: SQL Knowledge & Advanced SQL\n",
      "- **Day 1-3:** Introduction to SQL and Relational Database Concepts\n",
      "  - Overview of SQL languages, data types, and SQL commands (SELECT, INSERT, UPDATE, DELETE).\n",
      "- **Day 4-7:** Advanced SQL Techniques\n",
      "  - Joins, Subqueries, Indexing, Views, and Stored Procedures.\n",
      "- **Day 8-10:** Hands-on Project\n",
      "  - Create a small project using SQL with a mock dataset.\n",
      "\n",
      "#### Week 3: Introduction to Elasticsearch\n",
      "- **Day 1-3:** Setting Up Elasticsearch\n",
      "  - Installation and initial configuration; understanding nodes and clusters.\n",
      "- **Day 4-6:** CRUD Operations and Query DSL\n",
      "  - How to perform Create, Read, Update, and Delete operations; using Query DSL for searching.\n",
      "- **Day 7-10:** Hands-on Project\n",
      "  - Develop a simple application that indexes and queries data.\n",
      "\n",
      "#### Week 4: Debugging Techniques\n",
      "- **Day 1-3:** Java Debugging Tools and Techniques\n",
      "  - Understand IDE debugging tools, exception handling, and logging.\n",
      "- **Day 4-5:** Common Debugging Patterns\n",
      "  - Analyze real-world cases to troubleshoot and debug applications.\n",
      "- **Day 6-10:** Hands-on Debugging Challenge\n",
      "  - Participate in coding challenges with debugging scenarios.\n",
      "\n",
      "#### Week 5: Error Handling\n",
      "- **Day 1-3:** Understanding Error Handling in Java\n",
      "  - Explore Java exceptions, checked vs. unchecked exceptions, and best practices.\n",
      "- **Day 4-6:** Implementing Robust Error Handling\n",
      "  - Patterns for error handling: retries, fallbacks, and custom exceptions.\n",
      "- **Day 7-10:** Hands-on Project\n",
      "  - Refactor an existing Java application to implement robust error handling.\n",
      "\n",
      "#### Week 6: Capstone Project\n",
      "- **Day 1-5:** Integrate all skills learned\n",
      "  - Develop a full-fledged application that utilizes SQL, Elasticsearch, Java programming, and incorporates effective debugging and error handling.\n",
      "- **Day 6-7:** Present the project and receive feedback.\n",
      "\n",
      "### Additional Resources\n",
      "- **Books:** \n",
      "  - \"Effective Java\" by Joshua Bloch\n",
      "  - \"SQL for Data Analysis\" by Cathy Tanimura\n",
      "  - \"Elasticsearch: The Definitive Guide\" by Clinton Gormley and Zachary Tong\n",
      "\n",
      "- **Online Courses:**\n",
      "  - Platforms like Coursera, Udemy, and Pluralsight for advanced SQL, Elasticsearch, and Java programming.\n",
      "\n",
      "- **Tools:**\n",
      "  - IDE (e.g., IntelliJ IDEA) for Java development, SQL Server or MySQL for database work, and Kibana for Elasticsearch visualization.\n",
      "\n",
      "This bootcamp structure will help person1 develop the needed skills comprehensively to handle the issue effectively.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=api_key_)\n",
    "question= f\"Based on the users and their skills available in {user_skills_final_data}, to which user do you assign above issue that needs {skills}? In that dictionary, key is the name of the user.\"\n",
    "question1=f\"If you choose a user, what are the other skills required to master by user to complete that task effectively\"\n",
    "question2=f\"Provide a bootcamp to master those skills\"\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question1\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question2\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
